%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
% Now being used for CMU's 10704 Spring 2015 Information Processing and
% Learning course taught by Aarti Singh and Akshay Krishnamurthy.  When
% preparing LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi. "pdflatex template.tex" should also work.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}
\usepackage{hyperref}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 10-704: Information Processing and Learning
	\hfill Spring 2015} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept.}

   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may be distributed
   outside this class only with the permission of the Instructor.}
   \vspace*{-4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}       % Expectation
\newcommand\Exp{\mathcal{E}}    % Exponential Family
\renewcommand\L{\mathcal{L}}
\newcommand\Pds{\mathcal{P}}
\newcommand\R{\mathbb{R}}
\newcommand\X{\mathcal{X}}
\newcommand\Y{\mathcal{Y}}
\newcommand\sminus{\backslash}
\newcommand\nbr{\mathcal{N}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{6}{January 29}{Aarti Singh}{Shashank Singh}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Overview}

In the previous lecture we reviewed several entropy estimators for discrete
variables (plugin and LP estimators) and continuous variables (different plugin
and Von-Mises estimators). We extended this to estimate mutual information, and
then applied this to learning tree graphical models via the Chow-Liu algorithm.

In this lecture, we first discuss a procedure for learning more general
graphical models (the PC algorithm) using \emph{conditional} mutual information
estimators. We then switch to an unrelated topic: maximum entropy distributions
and information geometry/information projection.

\section{Application: Structure Learning in General Graphical Models}
Last time we discussed the Chow-Liu algorithm, which uses mutual information
estimation to learn the best tree graphical model representing data. Recall
that, in each iteration, the Chow-Liu algorithm greedily adds an edge between
the pair of unconnected variables exhibiting the greatest (estimated) mutual
information. Because we only add edges between unconnected variables,
(unconditional) mutual information suffices. However, to learn general
graphical models (allowing multiple paths between nodes), we need measure
conditional dependence, for which we can estimate conditional mutual
information.

In general, there are exponentially many subsets of $p$ variables on which we
might have to condition to learn a graphical model. However, by choosing our
conditional independence tests in a certain order, we reduce (by a polynomial
factor) the number of tests needed so as to be feasible for a moderate number
of variables.

By doing so, the PC algorithm,
\footnote{Peter-Clark Algorithm, named for proposers Peter Spirtes and Clark
Glymour \cite{PC00}; see also \cite{KB07} (available
\url{http://jmlr.csail.mit.edu/papers/volume8/kalisch07a/kalisch07a.pdf}) for a
more recent coverage.}
which uses a conditional independence test as a subroutine, gives an efficient
procedure for learning a general graphical model from joint observations of the
variables. Intuitively, the PC algorithm begins with a complete graph and
repeatedly picks an edge at random, removing it if it can find a set of
conditioning variables that the variables conditionally independent. See Figure
\ref{fig:PC_algo} for pseudocode.

\newpage
\begin{figure}
\begin{tabbing}
\hspace*{.15in} \= \hspace*{.15in} \= \hspace*{.15in} \= \hspace*{.15in} \= \hspace*{.15in} \= \hspace*{.15in} \=\kill
{\bf Inputs:}
\>\>\>\hspace{2mm}A set $\X = \{X_1,\dots,X_n\}$ of $p$ variables   \\
\>\>\>\hspace{2mm}A data set of $n$ joint observations
    $\{(x_{1,i},\dots,x_{p,i})\}_{i = 1}^n$ \\
\>\>\>\hspace{2mm}A test $T$ for conditional independence ($T(X_i,X_j,\Y) = TRUE$ iff
$X_i \perp X_j | \X$, for any $\Y \subseteq \X$) \\
{\bf Outputs:} An undirected graph $G = (\X,E)$ with $\{X_i,X_j\} \in E$ if and
only if $T(X_i,X_j,\Y) = FALSE$ \\
\>\>for every $\Y \subseteq \X$ with $X_i$ and $X_j$ connected in the graph
induced by $(\X,E\sminus\{X_i,X_j\})$ on $\X \sminus \Y$. \\\\ 
1) Initialize a complete graph $G = (\X,E)$ \\
2) Initialize $\ell = -1$   \\
3)\>{\bf while} $\ell$ is less than the maximum degree of $G$ \\
4)\>\>$\ell = \ell + 1$   \\
5)\>\>{\bf for each} edge $\{X_i,X_j\} \in E$ with
                                    $|\nbr_G(X_i) \sminus \{X_j\}| < \ell$  \\
6)\>\>\>{\bf for each} subset of neighbors
            $\Y \subseteq \nbr_G(X_i) \sminus \{X_j\}$ with $|\Y| = \ell$   \\
7)\>\>\>\>{\bf if} $T(X_i,X_j,\Y)$    \\
8)\>\>\>\>\>delete edge $\{X_i,X_j\}$ from $E$  \\
9)\>\>\>\>\>{\bf break}   \\
10)\>\>\>\>{\bf endif} \\
11)\>\>\>{\bf end for each} \\
12)\>\>{\bf end for each} \\
13)\> {\bf end while}
\end{tabbing}
\caption{Pseudocode of the PC algorithm. $\nbr_G(X_i)$ denotes the set of
neighbors of $X_i$ in $G$.}
\label{fig:PC_algo}
\end{figure}

\section{Maximum Entropy Density Estimation}
{\bf Motivation:}
We often consider the uniform or Gaussian distributions to be good priors
because they seem intuitively to be non-informative. This notion can be
formalized in sense that, uniform and Gaussian are maximum entropy
distributions: of all distributions satisfying certain constraints, they have
the greatest entropy. The uniform distribution arises when we constrain the
support of the distribution. The Gaussian appears when we constrain the first
two moments (mean and variance). Both of distributions belong to the
exponential family, which we will show is the family of solutions to the
following optimization problem:
\begin{align}
\label{opt_prob}
                            & \max_{p \in \Pds(\X)} H(p)                \\
\nonumber
\mbox{ subject to } \quad   & \E_{X \sim p}[f_i(X)] = \alpha_i,
                                            \qquad i \in \{1,\dots,n\}  \\
\nonumber
\mbox{ and } \quad          & \E_{X \sim p}[g_j(X)] \leq \beta_j,
                                            \qquad j \in \{1,\dots,m\},
\end{align}
where $\Pds(\X)$ is the set of probability densities on a sample space $\X$,
\footnote{The particular base measure $\mu$ on $\X$ is not important for the
theory, though, in applications, this must of course be specified, as shown in
the examples.}
each $f_i,g_j : \R \to \R$, and each $\alpha_i,\beta_j \in \R$.
This problem is natural in the following sense: if we estimate properties of a
distribution from data, a reasonable estimate of the distribution is be the
maximum entropy distribution with those properties. Theorem
\ref{thm:exp_fam_max_ent} parameterizes the solutions to this problem:

\begin{theorem}
The density $p^* \in \Pds(\X)$ solving the optimization problem \ref{opt_prob}
is in the exponential family
\[\Exp(\X)
    := \left\{ p : \X \to \R^+ :
            p(x) = \exp \left( -1 - \lambda_0
            + \sum_{i = 1}^n \lambda_i f_i(x)
            + \sum_{j = 1}^m \lambda_{n + j} g_j(x)
        \right),
    \qquad \forall x \in \R \right\},
\]
for some $\vec\lambda \in \R^{1 + n + m}$ with
$\lambda_{n + 1},\dots,\lambda_{n + m} \geq 0$. Furthermore, any
$p^* \in \Exp(\X)$ is a maximum entropy distribution (optimizes
\ref{opt_prob}), for some set of linear constraints.
\label{thm:exp_fam_max_ent}
\end{theorem}

\begin{proof}
\emph{Step 1.}
We first show, somewhat informally, that any maximum entropy distribution is in
$\Exp(\X)$.
\footnote{Since $H(p)$ is convex in $p$ and the constraints are linear in $p$,
this calculation can be made into a formal proof of optimality using methods
from the calculus of variations.}
If we rewrite the objective as minimizing $-H(p)$, then the Lagrangian
$\L : \Pds(\X) \times \left( \R^{n + 1} \times [0,\infty)^m \right) \to \R$ is
\[\L(p,\vec\lambda)
    = -H(p) + \lambda_0 \int_\X p(x) \, dx
    + \sum_{i = 1}^n \lambda_i \int_\X p(x) f_i(x) \, dx
    + \sum_{j = 1}^n \lambda_{n + j} \int_\X p(x) g_j(x) \, dx \;
    \footnote{We ignore some additive terms (e.g., $\alpha_i$'s and $\beta_j$'s
which is disappear when we take the derivative in $p(x)$.}
\]
(the $\lambda_0$ term comes from the implicit constraint
$\int_\X p(x) \, dx = 1$, since $p$ is a probability density).

Setting the derivative of the integrand with respect to $p(x)$ equal to $0$
gives, for the optimum $p^* \in \Pds(\X)$ and
$\vec\lambda \in \R^{1 + n + m}$,
\[0 
    = 1 + \log p^*(x) + \lambda_0
        + \sum_{i = 1}^n \lambda_i f_i(x)
        + \sum_{j = 1}^j \lambda_{n + j} g_j(x).
\]
Solving for $p^*(x)$ gives
\[p^*(x)
    = \exp\left( -1 - \lambda_0
        - \sum_{i = 1}^n \lambda_i f_i(x)
        - \sum_{j = 1}^j \lambda_{n + j} g_j(x).
    \right),
\]
which is the form of an exponential family distribution.

\emph{Step 2.} We now show any $p^* \in \Exp(\X)$ is a maximum entropy
distribution under appropriate constraints. For any $p \in \Pds(\X)$, first
applying Gibbs Inequality,
\begin{align}
H(p)
    = -\int_\X p(x) \log\left( \frac{p(x)}{p^*(x)} p(x) \right) \, dx
 &  = -D(p||p^*) - \int_\X p(x) \log p^*(x) \, dx
    \leq - \int_\X p(x) \log p^*(x) \, dx   \\
 &  = \int_\X p(x) \left( 1 + \lambda_0
                + \sum_{i = 1}^n \lambda_i f_i(x)
                + \sum_{j = 1}^m \lambda_{n + j} g_j(x)
        \right) \, dx   \\
\label{ineq:constraints}
 &  \leq \int_\X p(x) \left( 1 + \lambda_0
                + \sum_{i = 1}^n \lambda_i \alpha_i
                + \sum_{j = 1}^m \lambda_{n + j} \beta_j
        \right) \, dx   \\
 &  = \int_\X p^*(x) \left( 1 + \lambda_0
                + \sum_{i = 1}^n \lambda_i \alpha_i
                + \sum_{j = 1}^m \lambda_{n + j} \beta_j
        \right) \, dx   \\
\label{eq:comp_slack}
 &  = \int_\X p^*(x) \left( 1 + \lambda_0
                + \sum_{i = 1}^n \lambda_i f_i(x)
                + \sum_{j = 1}^m \lambda_{n + j} g_j(x)
        \right) \, dx   \\
 &  = \int_\X p^*(x) \log p^*(x) \, dx = H(p^*),
\end{align}
where (\ref{ineq:constraints}) follows from the constraints and
(\ref{eq:comp_slack}) follows from complementary slackness, since
\[\lambda^*_i (f_i(x) - \alpha_i) = \lambda_{n + j} (g_j(x) - \beta_j) = 0.\]
\end{proof}

\newpage
We now give a few examples of maximum entropy distributions under certain
constraints.

{\bf Example 1 (Uniform):} Suppose we constrain the domain $\E_{X \sim p}[1_A(X)] = 1$
for some $A \subseteq \X$ with $0 < \mu(A) < \infty$ for some base measure
$\mu$. Then, for some $\lambda_0,\lambda_1 \in \R$,
\[p^*(x) = \exp\left( -1 - \lambda_0 + \lambda_1 1_A(x) \right),\]
which is clearly uniform over $A$, and solving for $\lambda_0,\lambda_1$ from
the constraints gives $p^*(x) = \frac{1_A(x)}{\mu(A)}$, $\forall x \in \X$.

{\bf Example 2 (Exponential):} Suppose $\X = \R$ and we constrain the domain 
$\E_{X \sim p}[1_{[0,\infty)}(X)] = 1$ and the mean $\E_{X \sim p}[X] = \mu$.
Then, for some $\lambda_0,\lambda_1,\lambda_2 \in \R$,
\[p^*(x)
    = \exp\left( -1 - \lambda_0
                + \lambda_1 1_{[0,\infty)}(x) + \lambda_2 x
        \right),
\]
which is an exponential distribution, and solving for
$\lambda_0,\lambda_1,\lambda_2$ from the constraints gives
$p^*(x) = \frac{1}{\mu} e^{-x/\mu} 1_{[0,\infty)}$.

{\bf Example 3 (Gaussian):} Suppose $\X = \R$ and we constrain the mean
$\E_{X \sim p}[X] = \mu$ and the variance
$\E_{X \sim p}[(X - \mu)^2] = \sigma$. Then, for some
$\lambda_0,\lambda_1,\lambda_2 \in \R$,
\[p^*(x)
    = \exp\left( -1 - \lambda_0
                + \lambda_1 x + \lambda_2 (x - \mu)^2
        \right).
\]
Since $\E_{X \sim p}[X] = \mu$, $\lambda_1 = 0$. $p^*$ then has the form of a
Gaussian, and it follows that
$p^*(x)
    = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2} \right)$
(solving for $\lambda_0$ directly here involves some difficult integration).

\section{Information Geometry and Information Projection}

\emph{Information geometry} studies probability distributions geometrically,
considering the family $\Pds(\X)$ of probability densities on a sample space
$\X$ as (isomorphic to) a simplex in the space $[0,\infty)^{(|\X| - 1)}$ ($\X$
may be infinite). For example, the family of Bernoulli distributions is
isomorphic to the one-dimensional simplex. The next lecture will discuss the
geometric view of information geometry; here, we discuss an application.

An important problem in information geometry is \emph{information projection},
a generalization of the maximum entropy problem discussed above. The maximum
entropy problem can be viewed (in certain cases) as
\[p^*
    := \arg\max_{p \in Q} H(p)
    = \arg\min_{p \in Q} - H(p)
    = \arg\min_{p \in Q} \E_{X \sim p}[\log p(X)]
    = \arg\min_{p \in Q} D(p||u),
\]
where $Q \subseteq \Pds(\X)$ is a constraint set and $u$ is the uniform
distribution on $\X$; i.e., $p^*$ is the constrained distribution closest to
the uniform in KL-divergence. For a general distribution $p_0$, we can find
the constrained distribution closest to $p_0$; i.e.,
$p^* := \arg\min_{p \in Q} D(p||p_0)$. Under mild assumptions, the solution is
the \emph{Gibbs distribution} (a natural generalization of the exponential
family)
\[p^*(x)
    = p_0(x) \exp\left( -1 - \lambda_0 - \sum_{i = 1}^n \lambda_i f_i(x) \right)
    = \frac{p_0(x)e^{\sum_{i = 1}^n \lambda_i f_i(x)}}{Z_\lambda},
\]
where $Z_\lambda$ is a normalization constant. Next lecture, we will study this
via information geometric tools.

\section*{References}
\beginrefs
\bibentry{PC00}{\sc P.~Spirtes}, {\sc C.~Glymour} and {\sc R.~Scheines}, 
{\it Causation, Prediction, and Search.} The MIT Press, 2nd edition, 2000.
\bibentry{KB07}{\sc M.~Kalisch} and {\sc P.~B\"uhlmann}, 
``Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm,''
{\it Journal of Machine Learning Research 8}, 2007, pp.~613--636.
\endrefs

\end{document}
