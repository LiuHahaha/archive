\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{fullpage}

% For figures
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{amsmath,amssymb}

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%CONTENT MACROS%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\qed}{\quad \ensuremath{\blacksquare}}    % QED blacksquare
\newcommand{\inv}{^{-1}}                            % inverse operator
\newcommand{\sminus}{\backslash}                    % set minus
\newcommand{\N}{\mathbb{N}}                         % natural numbers
\newcommand{\R}{\mathbb{R}}                         % real numbers
\newcommand{\pow}{\mathcal{P}}                      % power set
\renewcommand{\H}{\mathcal{H}}                      % hypothesis class

\newcommand{\Se}{\mathcal{S}}                       % partition
\newcommand{\D}{\mathcal{D}}                        % partition
\newcommand{\e}{\varepsilon}                        % \varepsilon
\renewcommand{\d}{\delta}                           % \delta
\newcommand{\X}{\mathcal{X}}                        % X domain
\newcommand{\Y}{\mathcal{Y}}                        % Y domain
\newcommand{\Z}{\mathcal{Z}}                        % Z domain
\newcommand{\A}{\mathcal{A}}                        % sub-domain
\newcommand{\E}{\mathbb{E}}                         % expected value
\newcommand{\V}{\mathcal{V}}
\newcommand{\var}{\mathbb{V}}                       % variance
\newcommand{\pr}{\mathbb{P}}                        % probability
\newcommand{\hP}{{\hat P}}
\newcommand{\cpest}{\widehat{p}_h}                  % clipped estimated p_n
\newcommand{\cqest}{\widehat{q}_h}                  % clipped estimated q_n
\newcommand{\pest}{\widetilde{p}_h}                 % estimated p_n
\newcommand{\qest}{\widetilde{q}_h}                 % estimated q_n
\newcommand{\dist}{\operatorname{dist}}             % distance operator
\newcommand{\sgn}{\operatorname{sign}}              % sign operator
\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\ol}{\overline}
\renewcommand{\hat}{\widehat}
\newcommand{\poly}{\mathcal{P}}
\newcommand{\Unif}{\operatorname{Unif}}             % Uniform distribution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{natbib}
\usepackage[disable]{todonotes}

\begin{document}
\begin{center}
{\bf\Large Exact Post-Selection Inference for Forward Stepwise and Least Angle
Regression}\\
Shashank Singh\\
sss1@andrew.cmu.edu\\
\end{center}

Notes on the 2014 paper of the same name by Taylor, Lockhart, Tibshirani, and
Tibshirani.

\section{Introduction}
Consider observations $y \in \R^n$ drawn from a (not necessarily linear)
Gaussian model:
\begin{equation}
y = \theta + \e, \quad \e \sim \mathcal{N}(0,\sigma^2 I).
\label{eq:g_model}
\end{equation}
We discuss how to carry out exact inference after \emph{polyhedral selection},
i.e., selection characterized by $y \in \poly$ for some polyhedron
$\poly \subseteq \R^n$. We apply this to forward stepwise (FS) and least angle
regression (LAR), but the theory and methods are more general.
\footnote{As a motivating example, we consider prostate cancer data.}
Our selection adjusted $p$-values are exact for finite samples, and require
only that the predictor matrix $X$ be in general position.

\section{Summary of Results}
\subsection{Hypothesis Testing after Selection}
We would like to test the hypothesis $H_0 : v^T \theta = 0$, conditioned on
having observed $y \in \poly$, for some polyhedron $\poly \subseteq \R^n$
(represented as an intersection of half-spaces). We derive a test statistic
$T(y,\poly,v)$ such that, when $v^T \theta = 0$,
\[T(y,\poly,v) | y \in \poly \sim \Unif(0,1),\]
assuming only the Gaussian model (\ref{eq:g_model}).

As an example, suppose $A_k = [j_1,\dots,j_k]$ denotes the FS active list (in
order of selection) after $k$ steps, and $s_{A_k} = [s_1,\dots,s_k]$ denotes
the signs of the corresponding fitted coefficients. We will show that
\[\poly
    = \left\{ y \in \R^n : \hat A_k(y) = A_k,
                                            \hat s_{A_k} = s_{A_k} \right\}\]
is polyhedral, where $\hat A_k(y)$ and $\hat s_{A_k}$ denote the active list
and signs after $k$ steps as functions of $y$. If $X_{A_k} \in \R^{n \times k}$
denotes the submatrix of the covariate matrix $X$ with columns indexed by $A_k$
and $e_k$ denotes the $k^{th}$ canonical basis vector, then the above null
hypothesis becomes $H_0 : e_k^T (X_{A_k})^+ \theta = 0$,
\footnote{$M^+$ denotes the Moore-Penrose pseudoinverse of the matrix $M$.}
and, if $e_k^T (X_{A_k})^+ \theta = 0$, our statistic $T_k$ obeys
\[\pr\left[ T_k \leq \alpha | \hat A_k(y) = A_k, \hat s_{A_k} = s_{A_k} \right]
    = \alpha, \quad \alpha \in [0,1].\]

\subsection{Selection Intervals}
Our test statistic can also be inverted to make coverage statements about
linear contrasts of $\theta$. For example, for FS after $k$ steps, we derive a
selection interval $I_k$ such that
\begin{equation}
\pr\left[ e_k^T (X_{A_k})^+ | \hat A_k(y) = A_k,
                                            \hat s_{A_k}(y) = S_{A_k} \right]
    = 1 - \alpha.
\label{ineq:cond_selection_interval}
\end{equation}
Furthermore, we can marginalize (\ref{ineq:cond_selection_interval}) (over $y$)
to give an unconditional selection interval
\begin{equation}
\pr\left[ e_k^T (X_{A_k})^+ \right] \leq 1 - \alpha
\label{ineq:uncond_selection_interval}
\end{equation}
(which differs from a traditional confidence interval because $A_k$ is a random
set depending on $y$). While (\ref{ineq:cond_selection_interval}) is more in
line with the spirit of post-selection inference,
(\ref{ineq:uncond_selection_interval}) is, in a way, cleaner.

\subsection{Notation}
For a matrix $M \in \R^{n \times p}$ and an (ordered) list
$S \subseteq [1,\dots,p])$, $M_S \in \R^{n \times |S|}$ is the submatrix formed
by taking columns of $M$ indexed by $S$ (in order); similarly, for vectors in
$\R^p$. $(M^TM)^+$ denotes the Moore-Penrose pseudoinverse of the square matrix
$M^TM$, and $M^+ = (M^TM)^+M^T$ for a general matrix $M$. $P_L$ denotes the
projection operator onto a linear space $L$. For two vectors $x, y \in \R^n$,
$x \geq y$ should be read component-wise. $\Phi : \R \to \R$ denotes the
standard normal CDF, and for $a < b$,
\[F^{[a,b]}_{\mu,\sigma^2}(x)
    = \frac{\Phi\left( \frac{x - \mu}{\sigma} \right)
                                - \Phi\left( \frac{a - \mu}{\sigma} \right)}
        {\Phi\left( \frac{b - \mu}{\sigma} \right)
                                - \Phi\left( \frac{a - \mu}{\sigma} \right)}.
\]
denotes the CDF of $\mathcal{N}(\mu, \sigma^2)$ truncated to $[a,b]$.


\section{Conditional Gaussian inference after polyhedral selection}
The following lemma gives an alternative representation of a polygon $\poly$ in
terms of terms of an arbitrary selection vector $v \in \R^n$ (which may depend
on $\poly$). \\

{\bf Lemma 1 (Polyhedral selection as truncation):}
Suppose $y \sim \mathcal{N}(\theta,\Sigma)$, where $\theta \in \R^n$ is unknown
but $\Sigma \in \R^{n \times n}$ is known. Consider a polyhedron
$\poly = \{y \in \R^n : \Gamma y \geq u \}$, where $\Gamma \in \R^{m \times n}$
and $u \in \R^m$. If $v^T \Sigma v \neq 0$, then, for
\[\rho := \frac{\Gamma \Sigma v}{v^T \Sigma v},
    \quad \V^0(y) := \max_{j : \rho_j = 0} u_j - (\Gamma y)_j,\]
\[\V^{hi}(y) := \min_{j : \rho_j > 0}
    \frac{u_j - (\Gamma y)_j + \rho_j v^T y}{\rho_j},
    \quad \mbox{ and } \quad
\V^{lo}(y) := \max_{j : \rho_j < 0}
    \frac{u_j - (\Gamma y)_j + \rho_j v^T y}{\rho_j},\]
 $\V^0(y)$, $\V^{hi}(y)$, and $\V^{lo}(y)$ are independent of $v^T y$ and
\[y \in \poly
    \quad \Leftrightarrow \quad
    \V^0(y) \leq 0
    \mbox{ and }
    v^T y \in [\V^{lo}(y), \V^{hi}(y)].\]

The proof of Lemma 1 is trivial, noting that the $\V^0$ encodes constraints
parallel to $v$. \\

Since $v^T y$ has a Gaussian distribution, by Lemma 1, $v^T y | y \in \poly$
has a truncated Gaussian distribution. This gives rise to the following pivotal
statistic: \\

{\bf Lemma 2:} For $v^T \Sigma v \neq 0$ the statistic
\[F^{\V^{lo}(y), \V^{hi}(y)}_{v^T \theta, v^T \Sigma v}(v^T y) | y \in \poly
    \sim \Unif(0,1).\]

The proof of Lemma 2 is also fairly simple, and depends essentially on on the
independence of $\V^0,\V^{lo},\V^{hi}$ and $v^T y$. This gives rise to the
following two-sided conditional hypothesis test (testing $H_0 v^T \theta = 0$
against $H_1 : v^T \theta \neq 0$):
\footnote{Note that a slightly more powerful one-sided conditional test for the
alternative $H_1 : v^T \theta > 0$ is also available.} \\

{\bf Lemma 4 (Two-sided conditional inference after polyhedral selection:}
Suppose $v^T \Sigma v \neq 0$, and define the statistic
\[T := 2 \min \left\{ F^{\V^{lo}(y),\V^{hi}(y)}_{0,v^T \Sigma v}(v^T y),
            1 - F^{\V^{lo}(y),\V^{hi}(y)}_{0,v^T \Sigma v}(v^T y) \right\}.\]
Then, $T$ is a valid conditional $p$-value for $H_0$; i.e., under $H_0$,
$T | y \in \poly \sim \Unif(0,1)$. \\

\section{Selection-adjusted forward stepwise regression}
We use a version of FS that selects, in each iteration, the variable causing
the largest drop in residual error and assume that $X$ has columns in general
positions, which implies uniqueness of the selected variables and their signs.

\subsection{Details of the polyhedral set}
Let $A_k = [j_1,\dots,j_k]$ and $s_{A_k} = [s_1,\dots,s_k]$ denote the ordered
list of active variables and their signs (upon entering) after $k$ iterations.
Since FS greedily chooses variables which minimize the residual sum of squares
(RSS), for any $j \notin \{j_1,\dots,j_k\}$,
\begin{equation}
RSS(y,X_{A_k}) \leq R(y,X_{A_{k - 1} \cup \{j\}})
\label{ineq:FS_RSS}
\end{equation}
and
\begin{equation}
s_k = \sgn(e_k^T (X_{A_k})^+ y).
\label{eq:FS_sign}
\end{equation}

{\bf Theorem 1 (FS selection is polyhedral):} The set
\[\poly := \{y \in \R^n : \hat A_k(y) = A_k, \hat s_{A_k}(y) = s_{A_k}\}\]
of response vectors $y$ resulting in the FS active set $A_k$ and signs
$s_{A_k}$ is a polyhedron $\poly = \{y \in \R^n : \Gamma y \geq 0\}$. \\

\emph{Proof:}
We construct $\Gamma$ by adding $2(p - k)$ rows in for each iteration $k$ FS.
If $j_k$ and $s_k$ are the first variable and sign chosen by FS, then the
conditions (\ref{ineq:FS_RSS}) and (\ref{eq:FS_sign}) are equivalent
to
\[\frac{s_1X_{j_1}^T}{\|X_{j_1}\|_2} \geq \pm \frac{X_j^Ty}{\|X_j\|_2},
    \quad \forall j \notin A_k.\]
Thus, we add to $2(p - k)$ rows $\Gamma$, of the form
\[\frac{s_1X_{j_k}^T}{\|X_{j_1}\|_2} \pm \frac{X_j}{\|X)j\|_2},\]
(with one row for each pair of $\pm$ and $j \notin A_k$).

\subsection{Details of the tests and intervals}
To test a general hypothesis $H_0 : v^T \theta = 0$, we simply compute
$\V^{lo}(y),V^{hi}(y),V^0(y)$ as in Lemma 1 and then use the test statistic
\[T_k := 2\min\left\{ F^{[\V^{lo}(y),\V^{hi}(y)]}_{0,\sigma^2\|v\|_2^2}(v^T y),
        1 - F^{[\V^{lo}(y),\V^{hi}(y)]}_{0,\sigma^2\|v\|_2^2}(v^T y)\right\},\]
and, similarly, a $(1 - \alpha)$-confidence intervals for $v^T \theta$ is
$[\delta_{\alpha/2}, \delta_{1 - \alpha/2}]$ satisfying
\[1 - F^{[\V^{lo}(y),\V^{hi}(y)]}_{\delta_{\alpha/2},\sigma^2\|v\|_2^2}(v^T y)
    = \alpha/2\]
and
\[1 - F^{[\V^{lo}(y),\V^{hi}(y)]}
                    _{1 - \delta_{\alpha/2},\sigma^2\|v\|_2^2}(v^T y)
    = 1 - \alpha/2.\]

%{\small
%\bibliography{biblio}
%%\bibliographystyle{icml2014}
%\bibliographystyle{plain}
%}
\end{document}
