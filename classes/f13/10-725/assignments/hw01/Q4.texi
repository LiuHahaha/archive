Shashank Singh\footnote{sss1@andrew.cmu.edu}
\section{Convergence rate of subgradient method [25 points] (Adona)}
\begin{enumerate}[(a)]
\item (4 pts) Since the $2$-norm is induced by an inner product
$\langle\cdot,\cdot\rangle$,
\begin{align*}
\|x^{(k)} - x^\star\|_2^2
 & = \|x^{(k - 1)} - x^\star - t_kg^{(k - 1)}\|_2^2
                                    & \mbox{(def. of $x^{(k)}$)} \\
 & = \langle x^{(k - 1)} - x^\star - t_kg^{(k - 1)},
             x^{(k - 1)} - x^\star - t_kg^{(k - 1)} \rangle \\
 & = \|x^{(k - 1)} - x^\star\|_2^2
   - 2t_k \langle x^{(k - 1)} - x^\star,g^{(k - 1)}\rangle
   + t_k^2\|g^{(k - 1)}\|_2^2
                    & \mbox{(bilinearity of $\langle\cdot,\cdot\rangle$)} \\
 & \leq \|x^{(k - 1)} - x^\star\|_2^2
   - 2t_k \left(f(x^{(k - 1)}) - f(x^\star)\right)
   + t_k^2\|g^{(k - 1)}\|_2^2,
\end{align*}
where the inequality follows from the definition of a subgradient. \qed

\item (5 pts) If $g$ is a subgradient of $f$ at $x$, then by the Lipschitz
condition on $f$,
\begin{equation}
\|g\|_2^2
    = g^T(x + g - x)
    \leq f(x + g) - f(x)
    \leq G\|x + g - x\|_2 = G\|g\|_2,
\end{equation}
and so $\|g\| \leq G$. Thus, applying the recursive bound from (a) $k$ times
then gives
\begin{align*}
0   \leq \|x^{(k)} - x^\star\|_2^2
  & \leq \|x^{(0)} - x^\star\|_2^2
    + \sum_{i = 1}^k (-2t_i)\left(f(x^{(i - 1)}) - f(x^\star)\right)
    + t_i^2\|g^{(i - 1)}\|_2^2  \\
  & \leq R^2
    -2\sum_{i = 1}^k t_i\left(f(x^{(i - 1)}) - f(x^\star)\right)
    + G^2\sum_{i = 1}^k t_i^2. \qed
\end{align*}
\def \best {\ensuremath{_{\mbox{\scriptsize best}}}}
\item (4 pts) Since $x^{(k)}\best$ is chosen so as to minimize
$f(x^{(k)}\best)$ over $\{x^{(0)},\dots,x^{(k)}\}$,
\[2\sum_{i = 1}^k t_i\left(f(x^{(k)}\best) - f(x^\star)\right)
    \leq 2\sum_{i = 1}^k t_i\left(f(x^{(i - 1)}) - f(x^\star)\right)
    \leq R^2 + G^2\sum_{i = 1}^k t_i^2,\]
using a rearrangement of the result of part (b). Thus, further rearranging, we
have
\begin{equation}
\label{ineq:basic}
f(x^{(k)}\best) - f(x^\star)
\leq \frac{R^2 + G^2\sum_{i = 1}^k t_i^2}{2\sum_{i = 1}^k t_i}. \qed
\end{equation}

\item (4 pts) Plugging $t_1 = \dots = t_k = t$ into (\ref{ineq:basic}) and
taking the desired limit gives
\[
\lim_{k \to \infty} f(x^{(k)}\best) - f(x^\star)
    \leq \lim_{k \to \infty} \frac{R^2 + G^2kt^2}{2kt}
    = \mbox{\fbox{$\displaystyle \frac{G^2t}{2}$.}}
\]
Thus, the subgradient method with a constant step size $t$ converges to a point
at which the objective function exceeds its minimum by no more than $G^2t/2$.

\item (4 pts) Taking the desired limit in (\ref{ineq:basic}) gives
\[
\lim_{k \to \infty} f(x^{(k)}\best) - f(x^\star)
    \leq \lim_{k \to \infty} \frac{R^2 + G^2\sum_{i = 1}^k t_i^2}
                                  {2\sum_{i = 1}^k t_i}
    \leq \frac{R^2 + G^2\lim_{k \to \infty} \sum_{i = 1}^k t_i^2}
                                  {2\lim_{k \to \infty} \sum_{i = 1}^k t_i}
    = \mbox{\fbox{$0$.}}
\]
Thus the subgradient method with step sizes as specified converges to a minimum
of $f$.

\item (4 pts) Plugging $t_i = R/(G\sqrt k)$ into (\ref{ineq:basic}) gives
\begin{equation}
\label{ineq:tightestbound}
f(x^{(k)}\best) - f(x^\star)
    \leq \frac{R^2 + R^2k/k}{2k(R/G)\sqrt k}
    = RG k^{-3/2}.
\end{equation}

Since the $t_i$ was chosen to minimize (\ref{ineq:basic}), this is the best
bound we can derive from (\ref{ineq:basic}).
\end{enumerate}
