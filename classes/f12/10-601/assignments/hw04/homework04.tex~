\documentclass[11pt]{article}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{color}
\usepackage{graphicx}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\pagestyle{empty}

\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist

\newcounter{questionCounter}
\newcounter{partCounter}[questionCounter]
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
        \noindent{\bf #2}%
    \vspace{0.8em} \hrule \vspace{.10in}%
}{}

%%%%%%%%%%%%%%%%%%%%%%%HEADER%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\myname}{Shashank Singh}
\newcommand{\myandrew}{sss1@andrew.cmu.edu}
\newcommand{\myclass}{10-601 Machine Learning}
\newcommand{\myhwnum}{3}
\newcommand{\duedate}{Monday, October 15, 2012}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%CONTENT MACROS%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\qed}{\quad $\blacksquare$}
\newcommand{\mqed}{\quad \blacksquare}
\newcommand{\inv}{^{-1}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\pr}[1]{\mathsf{P}\left( #1 \right)} % probability of event #1
\newcommand{\Bern}[1]{\operatorname{Bernoulli}\left( #1 \right)} % Bernoulli distribution of parameter p
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\N}{\mathbb{N}} % natural numbers
\newcommand{\Q}{\mathbb{Q}} % rational numbers
\newcommand{\R}{\mathbb{R}} % real numbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\thispagestyle{plain}

{\Large Homework \myhwnum} \\
\myclass \\
Name: \myname \\
Email: \myandrew \\
Due: \duedate \\

\section{Neural Networks [25 points]}
\subsection{Expressiveness of Neural Networks [10 points]}
\begin{enumerate}[1.]
\item Note that $Y > 0.5$ if and only if $w_0 + \sum_i w_iX_i > 0$. Thus, as
shown in the below table, for $w_0 = 0, w_1 = w_2 = 1$, $Y$, when interpreted
as a boolean, obeys $Y = X_1 \vee X_2$:
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 $X_1$ & $X_2$ & $w_0 + \sum_i w_iX_i$ & $Y$     & $Y$ interpreted as a boolean \\
\hline
 $0$   & $0$   & $0$                   & $0.5$   & $0$                          \\
\hline
 $0$   & $1$   & $1$                   & $> 0.5$ & $1$                          \\
\hline
 $1$   & $0$   & $1$                   & $> 0.5$ & $1$                          \\
\hline
 $1$   & $1$   & $2$                   & $> 0.5$ & $1$                          \\
\hline
\end{tabular}
\label{tab:1}
\end{table}


\item As shown in the below table, for $w_0 = -1, w_1 = w_2 = 1$, $Y$, when
interpreted as a boolean, obeys $Y = X_1 \wedge X_2$:
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 $X_1$ & $X_2$ & $w_0 + \sum_i w_iX_i$ & $Y$     & $Y$ interpreted as a boolean \\
\hline
 $0$   & $0$   & $-1$                  & $< 0.5$ & $0$                          \\
\hline
 $0$   & $1$   & $0$                   & $0.5$   & $0$                          \\
\hline
 $1$   & $0$   & $0$                   & $0.5$   & $0$                          \\
\hline
 $1$   & $1$   & $1$                   & $> 0.5$ & $1$                          \\
\hline
\end{tabular}
\label{tab:2}
\end{table}

\item Using the weights $W_{N_1} = (-1, 1.25, 1.25)$ for Unit 1 and
the weights $W_{N_2} = (-1,0.75,0.75)$ for Unit 2 in the below neural network
results in $Y = X_1 \oplus X_2$ when $Y$ is interpreted as a boolean.
\vspace{2in}

\item Using the weights $W_{N_1} = (-1, -0.3, 1.2, -0.15, -0.15)$ for Unit 1
and the weights \\ $W_{N_2} = (-1, 0.3, -0.3, 0.6, 0.6)$ for Unit 2 in the below
neural network with one hidden layer results in
$Y = (A \vee \neg B) \oplus (\neg C \vee \neg D)$ when $Y$ is interpreted as a
boolean.
\vspace{2in}

\end{enumerate}

\subsection{MCLE, MAP, Gradient descent [15 points]}
\begin{enumerate}[1.]
\item Since the weights are assumed to be independent,
\begin{align*}
\ln \left( \pr{W} \right)
 & = \ln \left( \prod_i \pr{w_i} \right)
   = \ln \left( \prod_i \frac{1}{\sigma\sqrt{2\pi}}\exp\left( \frac{-w_i^2}{2\sigma^2} \right) \right) \\
 & = \sum_i \ln \left( \frac{1}{\sigma\sqrt{2\pi}}\exp\left( \frac{-w_i^2}{2\sigma^2} \right) \right) \\
 & = \sum_i \left( \ln \left( \exp \left( \frac{-w_i^2}{2\sigma^2} \right) \right) - \ln \left( \sigma \sqrt{2\pi} \right) \right) \\
 & = \sum_i \left( \frac{-w_i^2}{2\sigma^2} - \ln \left( \sigma \sqrt{2\pi} \right) \right)
   = -\sum_i \left( cw_i^2 + \ln \left( \sigma \sqrt{2\pi} \right) \right), \\
\end{align*}
for $c = \frac{1}{2\sigma^2}$. Thus, by monotonicity of the logarithm the
weights maximizing $\pr{W}$ are the same as the weights minimizing
$c\sum_i w_i^2$. As shown in lecture for MLE, the weights maximizing
$\prod_l \pr{Y^; | X^l; W}$ are those minimizing
$\sum_l (y^l - \hat{f}(x^l))^2$. Thus,
\[\argmax_W \ln \pr{W} \prod_l \pr{Y^l | X^l; W}
 = \argmin_W c\sum_i w_i^2 + \sum_l (y^l - \hat{f}(x^l))^2. \mqed \]

\item As shown in lecture,
\[\frac{\partial }{\partial w_i}\left( \sum_l (y^l - \hat{f}(x^l))^2 \right)
 = \sum_l (y^l - \hat{f}(x^l)) \hat{f}(x^l)(1 - \hat{f}(x^l)) x_i^l.\]
Thus, by linearity of the derivative,
\[\frac{\partial E}{\partial w_i}
 = \frac{\partial}{\partial w_i} \left( c\sum_i w_i^2 \right)
 + \frac{\partial }{\partial w_i}\left( \sum_l (y^l - \hat{f}(x^l))^2 \right)
 = \mbox{\fbox{$\displaystyle 2cw_i - \sum_l (y^l - \hat{f}(x^l)) \hat{f}(x^l)(1 - \hat{f}(x^l)) x_i^l$.}}
\]
\end{enumerate}

\section{Bayesian Networks [20 Points]}
\subsection{Representation and Inference [12 points]}
\begin{enumerate}[1.]
\item By definition of conditional probability,
\begin{align*}
\pr{A,B,C,D}
 & = \pr{C | A,B,D} \pr{A,B,D}            \\
 & = \pr{C | A,B,D} \pr{B | A,D} \pr{A,D} \\
 & = \pr{C | A,B,D} \pr{B | A,D} \pr{A,D} \\
 & = \pr{C | B,D} \pr{B | A,D} \pr{A} \pr{D}.
\end{align*}
Then, since the events not connected by edges in the network are assumed to be
conditionally independent,
\[\pr{A,B,C,D} = \mbox{\fbox{$\pr{C | B,D} \pr{B | A} \pr{A} \pr{D}$.}}\]

\item The independent parameters needed are: \\
\begin{center}
\begin{tabular}{lll}
$\pr{C = 0 | B = 0,D = 0}$ & \quad & $\pr{C = 0 | B = 0,D = 1}$ \\
$\pr{C = 0 | B = 1,D = 0}$ & \quad & $\pr{C = 0 | B = 1,D = 1}$ \\
$\pr{B = 0| A = 0}$        & \quad & $\pr{B = 0| A = 1}$        \\
$\pr{A}$                   & \quad & $\pr{D}$,                  \\
\end{tabular} \\
\end{center}

so that \fbox{$8$} independent parameters are needed.

\item If no independence or conditional independence assumptions are made,
values for $P(A,B,C,D)$ are needed for all but one of the $2^4$ possible
boolean assignments to $A,B,C,D$ (since the sum of the probabilities is $1$).
Thus, $2^4 - 1 = $ \fbox{$15$} parameters are required.

\item Define
\[p = \pr{B = 1} = \pr{B = 1 | A = 0}\pr{A = 0} + \pr{B = 1 | A = 1} \pr{A = 1}.\]
By Bayes' Rule, and then by definition of conditional probability,
\begin{align*}
 & P(B = 1 | C = 0) \\
 & = \frac{\pr{C = 0 | B = 1}\pr{B = 1}}{\pr{C = 0}} \\
 & = \frac{\pr{C = 0 | B = 1}\pr{B = 1}}
          {\pr{C = 0 | B = 0} \pr{B = 0} + \pr{C = 0 | B = 1}\pr{B = 1}} \\
 & = \mbox{\fbox{$\displaystyle
     \frac{(1 - \pr{C = 1 | B = 1}) p}
          {(1 - \pr{C = 1 | B = 0}) (1 - p) + ( 1 - \pr{C = 1 | B = 1})p}$,}} \\
\end{align*}
noting that $p$ is written in terms of the network parameters.

\end{enumerate}

\subsection{Learning Bayes Nets [8 points]}
\begin{enumerate}[1.]
\item $BN$ should be used to model the dataset $A$, because adding edge
between $X_1$ and $X_2$ would increase the number parameters that need to be
estimated for our network, without any justifiable representation by which to
model the relationship between $X_1$ and $X_2$ (this is especially problematic
when $A$ has a small number of training examples).

\item $BN^{\prime}$ should be used to model dataset $B$, because the edge
from $X_1$ to $X_2$ allows us take into account the relationship
$x_2^j = F(x_1^j, \theta) + \epsilon$ between $X_1$ and $X_2$ to increase our
accuracy.
\end{enumerate}

\section{Expectation Maximization (EM) [15 points]}
\begin{enumerate}[1.]
\item During the first E step, EM calculates the expected value of the sole
unobserved variable, $B_2$ (the value of $B$ in the $2^{nd}$ training
example).

\item Since $B_2$ is distributed according to a Bernoulli distribution, if \\
$\theta = \pr{B = 1 | A = 1} = 1 - \pr{C = 0 | B = 1}$,
\begin{align*}
E[B_2]
 & = \pr{B_2 = 1} \\
 & = \frac{\pr{B = 1, A = 1, C = 0 | \theta = 0.6}}
        {\pr{B = 1, A = 1, C = 0 | \theta = 0.6} + \pr{B = 0, A = 1, C = 0 | \theta = 0.6}} \\
 & = \frac{0.6^2 \cdot 0.4}{0.6^2 \cdot 0.4 + 0.6 \cdot 0.4^2}
   = \mbox{\fbox{$0.6$.}}
\end{align*}

\item During the first M step, EM updates the parameters of parameters of the
Bayes network, (those enumerated in question 2).

%TODO
\item The parameters are updated as follows:
\begin{align*}
P(A = 1) = \frac{2}{4} & = \mbox{\fbox{$\displaystyle \frac{1}{2}$.}} \\
P(B = 1 | A = 0) = \frac{1}{2} & = \mbox{\fbox{$\displaystyle \frac{1}{2}$.}} \\
P(B = 1 | A = 1) = \frac{1.6}{2} & = \mbox{\fbox{$\displaystyle \frac{4}{5}$.}} \\
P(C = 1 | B = 0) = \frac{1}{1.4} & = \mbox{\fbox{$\displaystyle \frac{5}{7}$.}} \\
P(C = 1 | B = 1) = \frac{1}{2.6} & = \mbox{\fbox{$\displaystyle \frac{5}{13}$.}} \\
\end{align*}
\end{enumerate}

\section{Midterm Review Questions [15 points]}
\subsection{True or False Questions [9 points]}
\begin{enumerate}[a.]
\item \fbox{True;} as the number of training examples goes to infinity, the
effect of a fixed number of ``hallucinated'' examples (the prior) tends to
zero.

\item \fbox{True;} overfitting occurs when too strong a hypothesis is guessed
based on insufficient data; equivalently, when fitting data of the form
$Y = F(X) + \epsilon$, where $\epsilon$ is a noise-term, the effect of
$\epsilon$ on the hypothesis tends to zero as the number of training examples
goes to infinity.

\item \fbox{False;} a decision tree partitions the domain on based on each
attribute; with noise-free data, the order in which the domain is partitioned
does not affect the resulting partitions.
\end{enumerate}

\subsection{Short Questions [6 points]}
\begin{enumerate}[a.]
\item By Bayes' Rule,
\[\argmax_c \pr{c | x}
 = \argmax_c \frac{\pr{x | c} \pr{c}}{\pr{x}}
 = \argmax_c \pr{x | c} \pr{c}.\]
$\argmax_c \pr{x | c} \pr{c} = \argmax_c \pr{x | c}$ precisely \fbox{when
$\pr{c}$ is uniformly distributed.}

\item I would use a \fbox{neural network} with no hidden layer, since this can
be made to resemble logistic regression, which is a linear classifier. A
decision tree, on the other hand, is, in general, a non-linear classifer.

\end{enumerate}
\end{document}
