\begin{abstract}
Estimating divergences in a consistent way is of great importance in many machine learning
tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge
there has been no finite sample exponential inequality convergence bound derived for any divergence estimators.
The main contribution of our work is to provide such bound for a smooth Ho\"lder class of densities in $d$-dimensional Euclidean spaces. We also demonstrate the applicability of the estimator in mutual information estimation and illustrate our theoretical results with numerical experiments.
\end{abstract}

\section{Introduction}
There are several important problems in machine learning and statistics that
require the estimation of the divergence between two distributions. In the past
few decades many different kind of divergences have been defined between
distributions. They include the Kulback--Leibler (\acro{KL}) \citep{kullback51KL}, R\'enyi-$\alpha$
\citep{renyi61measures,renyi70probability}, Tsallis-$\alpha$
\citep{villmann10mathematical}, Bregman \citep{bregman67divergence}, Jensen--Shannon \citep{lin91divergence}, $L_p$, maximum mean
discrepancy \citep{Borgwardt06MMD}, Csisz\'ar's-$f$ divergence \citep{csiszar67information} and many
others.

Divergences can be used to generalize regression, classification, clustering and  many other machine learning problems from feature vector inputs to sets and distributions \citep{poczos12kernelimages,oliva13ICML}.
Under certain conditions, divergences can estimate entropy and mutual
information. Entropy estimators are important in goodness-of-fit testing
\citep{vasicek76test,goria05new}, parameter estimation in semi-parametric
models \citep{Wolsztynski85minimum}, studying fractal random walks
\citep{Alemany94fractal}, and texture classification
\citep{hero2002aes,hero02alpha}.  Mutual information estimators have been used
in feature selection \citep{peng05feature},
clustering \citep{aghagolzadeh07hierarchical},
causality detection \citep{Hlavackova07causality},
optimal experimental design \citep{lewi07realtime}, %poczos09identification
f\acro{MRI} data processing \citep{chai09exploring},
prediction of protein structures \citep{adami04information},
and boosting and facial expression recognition \citep{Shan05conditionalmutual}.
Both entropy estimators and mutual information estimators have been used for
independent component and subspace analysis
\citep{radical03,Hulle08constrained,szabo07undercomplete_TCC}, %poczos05geodesic
as well as for image registration
\citep{kybic06incremental,hero2002aes,hero02alpha}.
For further applications, see
\citet{Leonenko-Pronzato-Savani2008,WKV2009Survey}.

 
In this paper we will focus on the estimation of R\'enyi-$\alpha$ divergences.
This important class contains the Kullback--Leibler divergence as the
$\alpha\to 1$ limit case and can also be related to the Tsallis-$\alpha$,
Jensen-Shannon, and Hellinger divergences.\footnote{Some of the divergences
mentioned in the paper are distances as well. To simplify the treatment we will
call all of them divergences.}
It can be shown that many information theoretic quantities
(including entropy, conditional entropy, mutual information, and divergence)
can be computed as special cases of Renyi $\alpha$-divergence with an at most
constant-factor increase in error.




In our framework, we assume that the underlying distributions are not given explicitly. Only two
finite, independent and identically distributed (i.i.d.) samples are given from
some unknown, continuous, nonparametric distributions. Although many of the above mentioned divergences were
defined a couple of decades ago, interestingly there are still many open
questions left to be answered about the properties of their estimators. In
particular, even simple questions, such as rates are unknown for many
estimators, and to the best of our knowledge no finite sample exponential concentration bounds have ever been derived for divergence estimators.

\textbf{Our main contribution} is to derive an exponential concentration bound for a particular consistent, nonparametric, R\'enyi-$\alpha$ divergence estimator. We illustrate the behaviour of the estimator with numerical experiments and demonstrate how it can be applied for mutual information estimation. 

After publication of our results, we will make a Matlab implementation of the divergence estimator publicly available.
 
\subsection*{Organization}
In the next section we discuss related work (Section~\ref{sec:related}).
In the Section~\ref{sec:Problem} we formally
define our estimation problem, introduce the R\'enyi-$\alpha$ divergence, and
introduce the notation and the assumptions used in the paper. Section~\ref{sec:estimator} presents the divergence estimator analyzed in the paper. Section~\ref{sec:main-result} contains our main theoretical results
about the exponential-concentration bound of the divergence estimator.
Section~\ref{sec:Proofs} contains the proofs of our theorems. The demonstrate the applicability of the estimator, we provide a few simple numerical experiments in Section~\ref{sec:numerical}. Here we also demonstrate how the divergence estimator can be used for estimation mutual information as well.
We finish the paper with a short discussion and drew conclusion.



\section{Related Work} \label{sec:related}

Probably the closest work to our contribution is \citet{liu12exponential},
who derive exponential-concentration bound for an estimator of the dimensional shannon entropy. 
We generalize these results in several aspects.
\begin{enumerate}
\item The estimator of \citet{liu12exponential} operates in the unit square $[0,1]^2$. Our
estimator operates on the $d$-dimensional unit hypercube $[0,1]^d$.
\item In \citet{liu12exponential} the exponential concentration inequality was proven for
densities in the H\"{o}lder class $\Sigma_\kappa(2,L,2)$, whereas our inequality
applies for densities in the H\"{o}lder class $\Sigma_\kappa(\beta,L,r)$ for
any fixed $\beta \geq 0, r \geq 1$ (the estimator converges as
$O\left( n^{-\frac{\beta}{d + \beta}} \right)$).
\item While \citet{liu12exponential} estimated Shannon entropy using one i.i.d. sample set, in this paper
we estimate the Renyi $\alpha$-divergence using two i.i.d. sample sets.

\end{enumerate}

<<<<<<< .mine
After publication of the paper, we will make a matlab implementation publicly
available to help reproducing our results.
=======
>>>>>>> .r15

<<<<<<< .mine
 
\subsection*{Organization}
In the Section~\ref{sec:Problem} we formally define our estimation problem,
introduce the R\'enyi-$\alpha$ divergence, and introduce the notation and the
assumptions used in the paper. Section~\ref{sec:estimator} presents the
divergence estimator analyzed in the paper. Section~\ref{sec:main-result}
contains our main theoretical results about the exponential-concentration bound
of the divergence estimator.  Section~\ref{sec:Proofs} contains the proofs of
our theorems. The demonstrate the applicability of the estimator, we provide a
few simple numerical experiments in Section~\ref{sec:numerical}. Here we also
demonstrate how the divergence estimator can be used for estimation of mutual
information. We finish the paper with a short discussion and conclusion.
=======
>>>>>>> .r15

\citet{poczos11AISTATS} and proposed a $k$-nearest neighbour based R\'enyi-$\alpha$ divergence estimator. 
They proved the weak consistency of the estimator, but they did not study the convergence rate.

<<<<<<< .mine
\section{Related Work}
AISTATS Renyi divergence estimator paper
=======
>>>>>>> .r15
 \citet{Wang-Kulkarni-Verdu2009} provided an estimator for
the $\alpha \to 1$ limit case only, i.e., for the
\acro{KL}-divergence. They did not study the convergence rate either, and 
there is also an apparent error in this work; they applied the reverse Fatou lemma under
conditions when it does not hold. This error originates in the work
 \citet{kozachenko87statistical} and can also be found in other
works. Recently, \citet{perez08estimation} has proposed an other
consistency proof for this estimator, but it also contains some
errors: he applies the strong law of large numbers under conditions
when it does not hold
 and also assumes that convergence in
probability implies almost sure
convergence.

\citet{hero02alpha,hero2002aes} also investigated the
R\'enyi divergence estimation problem but assumed that one of the two
density functions is known. \citet{gupta12parametric} developed
algorithms for estimating the Shannon entropy and the \acro{KL}
divergence for certain parametric
families.


Recently, \citet{nguyen09surrogate,nguyen10estimating}
developed methods for estimating $f$-divergences using their
variational characterization properties. They estimate the likelihood
ratio of the two underlying densities and plug that into the
divergence formulas. This approach involves solving a convex
minimization problem over an infinite-dimensional function space. For
certain function classes defined by reproducing kernel Hilbert spaces
(\acro{RKHS}), however, they were able to reduce the computational
load from solving infinite-dimensional problems to solving
$n$-dimensional problems, where $n$ denotes the sample size. When $n$
is large, solving these convex problems can still be very demanding.
Furthermore, choosing an appropriate \acro{RKHS} also introduces
questions regarding model selection. An appealing property of our
estimator is that we do not need to solve minimization problems over
function classes.

<<<<<<< .mine
Recently, \citet{2010arXiv1012.4188S} proposed $k$-nearest-neighbor based
methods for estimating non-linear functionals of density, but in contrast to
our approach, they were interested in the case where $k$ increases with the
sample size.
=======
\citet{2010arXiv1012.4188S,Laurent96Efficient,birge95estimation} studied 
the estimation of non-linear
functionals of density. 
 They, however, did not study the Renyi divergence estimation and did not derive exponential concentration bounds either. Using ensemble estimators, \citet{sricharan12ensemble} derived fast rates for entropy, but they did not investigate the divergence estimation problem.
>>>>>>> .r15

<<<<<<< .mine
\todo{Also check the paper Csaba has sent + Nikolai's other papers + paper
(parametric) from Gabor!}
=======
>>>>>>> .r15

\citet{Leonenko-Pronzato-Savani2008} and \citet{goria05new}
considered Shannon and R\'enyi-$\alpha$ entropy estimation from a
single sample.\footnote{The original presentations of these works
contained some errors; \citet{leonenko10correction} provide
corrections for some of these theorems.} In contrast, we propose
divergence estimators using two independent samples.   Recently,
\citet{Poczos-Kirshner-Szepesvari2010,pal10estimation} proposed a
method for consistent R\'enyi information estimation, but this
estimator also uses one sample only and cannot be used for estimating
divergences. Further information and useful reviews of several
different divergences can be found, e.g.,
in \citet{villmann10mathematical}, \citet{cichocki09nonnegative}, and
\citet{WKV2009Survey}.

 